{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "<font size=\"1\">\n",
    "Supplementary code for \"Build a Large Language Model From Scratch\": <a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">https://www.manning.com/books/build-a-large-language-model-from-scratch</a> by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# Chapter 5: 在未标记数据上进行预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.3\n",
      "numpy version: 1.23.5\n",
      "tiktoken version: 0.6.0\n",
      "torch version: 2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \"numpy\", \"tiktoken\", \"torch\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- 在本章中，我们将实现训练循环及基本模型评估代码，以预训练一个LLM\n",
    "- 本章结尾处，我们还将加载OpenAI提供的公开可用的预训练权重并将其导入到我们的模型中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"images/img-1.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214765-7a73-42d5-95e9-302154b29db9",
   "metadata": {},
   "source": [
    "- 本章所涵盖的主题如下图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"images/img-2.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "## 5.1 评估文本生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- 我们首先简要回顾一下使用上一章中的代码初始化 GPT 模型\n",
    "- 然后，我们讨论 LLM 的基本评估指标\n",
    "- 最后，在本节中，我们将这些评估指标应用于训练和验证数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "### 5.1.1 使用 GPT 生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- 我们使用上一章中的代码初始化 GPT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"ctx_len\": 256,       # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- 我们在上面使用0.1的dropout，但现在的llm训练中通常没有dropout\n",
    "- 现在的llm也不会在查询，键和值矩阵的`nn.Linear`层中使用偏差向量 (与早期的GPT模型不同)，这是通过设置`“qkv_bias”: False`来实现的\n",
    "- 我们只用256个token的上下文长度 (`ctx_len`)，以减少训练模型的计算资源需求，而原始的1.24亿参数GPT-2模型使用1024个token\n",
    "  - 这是为了让更多的读者能够在他们的笔记本电脑上执行代码示例\n",
    "  - 但是，请随意增加`ctx_len`到1024token (这不需要任何代码更改)\n",
    "  - 之后我们还将从预训练的权重加载具有`ctx_len = 1024`的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- 接下来，我们使用上一章中的`generate_text_simple`函数来生成文本。\n",
    "- 此外，我们定义了两个函数`text_to_token_ids`和`token_ids_to_text`，用于在本章中进行标记和文本表示之间的转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"images/img-3.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # 增加batch维度\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # 去掉batch维度\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- 如上所述，模型未能生成好的文本，因为它尚未经过训练。\n",
    "- 我们如何以数值形式衡量或捕捉“好的文本”，以便在训练过程中进行跟踪？\n",
    "- 下一小节将介绍用于计算生成输出的损失指标的度量标准，我们可以使用这些度量标准来衡量训练进度。\n",
    "- 在后续关于微调大型语言模型（LLMs）的章节中，也将介绍其他衡量模型质量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
   },
   "source": [
    "### 5.1.2 计算文本生成损失：交叉熵和困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- 假设我们有一个`inputs`张量，包含了2个训练样本（行）的标记ID。\n",
    "- 对应于`inputs`，`targets`包含了我们希望模型生成的期望标记ID。\n",
    "- 请注意，`targets`是`inputs`向右移动了一个位置，正如第2章中实现数据加载器时所解释的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [588,  428,  11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- 将`inputs`输入模型后，我们获得了包含3个标记的2个输入样本的logits向量。\n",
    "- 每个标记都是一个50,257维的向量，对应于词汇表的大小。\n",
    "- 应用softmax函数，我们可以将logits张量转换为一个相同维度的张量，其中包含概率分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # 词表中每个标记的预测概率\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- 下图为了说明目的使用了一个非常小的词汇表，概述了我们如何将概率分数转换回文本，这一点我们在上一章的末尾进行了讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- 正如在前一章中讨论的，我们可以应用`argmax`函数将概率分数转换为预测的标记ID。\n",
    "- 上文提到的softmax函数为每个标记生成了一个50,257维的向量；`argmax`函数返回这个向量中最高概率分数的位置，即给定标记的下一个预测标记ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- 由于我们有2个输入批次，每个批次包含3个标记，因此我们获得了2个3维的预测标记ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- 如果我们解码这些标记，我们会发现它们与我们希望模型预测的标记，即目标标记，相当不同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- 那是因为模型还没有被训练。\n",
    "- 为了训练模型，我们需要知道它离正确预测（目标）有多远。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- 对应于目标索引的标记概率如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Batch 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "target_probas_1 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\n",
    "print(\"Batch 1:\", target_probas_1)\n",
    "\n",
    "batch_idx = 1\n",
    "target_probas_2 = probas[1, [0, 1, 2], targets[1]]\n",
    "print(\"Batch 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- 我们希望最大化所有这些值，使它们接近1的概率。\n",
    "- 在数学优化中，最大化概率分数的对数比分数值本身更容易；这超出了本书的范围，但我在这里录制了一个更详细的讲座：[L8.2 逻辑回归损失函数](https://www.youtube.com/watch?v=GxJe0DZvydM)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# 计算所有标记的预测概率的对数值\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- 接下来，我们计算平均对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7722)\n"
     ]
    }
   ],
   "source": [
    "# 对所有标记的概率对数值求均值\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- 目标是通过优化模型权重，使得这个平均对数概率尽可能大。\n",
    "- 由于对数函数的特性，最大可能的值是0，而我们目前远离0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- 在深度学习中，我们通常不是最大化平均对数概率，而是遵循标准惯例来最小化平均对数概率的*负值*；在我们的例子中，不是最大化-10.7722使其接近0，在深度学习中，我们会最小化10.7722使其接近0。\n",
    "- 负-10.7722的值，即10.7722，在深度学习中也被称为交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch 已经实现了一个 `cross_entropy` 函数，该函数执行了前面的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- 在我们应用交叉熵函数之前，让我们先检查一下logits和targets的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits向量的形状 (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# 目标向量的形状 (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- 对于PyTorch中的`entropy_loss`函数，我们希望通过在批次(batch)维度上合并它们来展平(flatten)这些张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- 请注意，目标（targets）是标记ID，它们也代表了我们希望在logits张量中最大化的索引位置。\n",
    "- PyTorch中的`cross_entropy`函数会自动地将softmax和对数概率计算应用到这些要最大化标记索引的logits上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- 一个 与交叉熵损失相关的概念是大型语言模型（LLM）的困惑度。\n",
    "- 困惑度简单地说就是交叉熵损失的指数函数计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47678.8633)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- 困惑度通常被认为更具可解释性，因为它可以被理解为模型在每一步中对下一标记所不确定的词表大小（在上面的例子中，这将是47,678个单词或标记）。\n",
    "- 换句话说，困惑度提供了一种衡量模型预测的概率分布与数据集中单词实际分布匹配程度的方法。\n",
    "- 与损失类似，较低的困惑度表明模型预测更接近实际分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "### 5.1.3 计算训练集和验证集损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- 我们使用一个相对较小的数据集来训练大型语言模型（LLM）（实际上，只有一个短篇故事）。\n",
    "  - 原因包括：\n",
    "    - 你可以在没有合适GPU的笔记本电脑上在几分钟内运行代码示例。\n",
    "    - 训练完成得相对较快（几分钟而不是几周），这对我们的教育目的来说很好。\n",
    "    - 我们使用的是公有领域的文本，可以包含在这个GitHub仓库中而不会违反任何使用权或增加仓库大小。\n",
    "\n",
    "- 例如，Llama 2 7B在A100 GPU上需要184,320小时的训练时间才能在2万亿个标记上完成训练。\n",
    "  - 在撰写本文时，AWS上8xA100云服务器的每小时成本大约为30美元。\n",
    "  - 因此，通过一个粗略的计算，训练这个LLM的成本将是 184,320 / 8 * 30美元 = 69万美元。\n",
    "\n",
    "- 下面，我们将使用第2章中使用过的相同数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://github.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- 通过打印前100个和后100个单词来快速检查文本是否正确加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_char = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_char)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- 虽然只有5,145个标记，对于训练一个大型语言模型（LLM）来说，这段文本非常短，但再次强调，这是出于教育目的（我们稍后还会加载预训练的权重）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- 接下来，我们将数据集划分为训练集和验证集，并使用第2章中的数据加载器为大型语言模型（LLM）训练准备批次数据。\n",
    "- 为了可视化目的，下面的图表假设`max_length=6`，但对于训练加载器，我们将`max_length`设置为LLM支持的上下文长度。\n",
    "- 下面的图表仅显示输入标记以简化表示。\n",
    "  - 由于我们训练LLM来预测文本中的下一个单词，目标标记看起来与这些输入标记相同，只是目标标记向右移动了一个位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# 训练集/验证集数据比\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n",
    "    stride=GPT_CONFIG_124M[\"ctx_len\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n",
    "    stride=GPT_CONFIG_124M[\"ctx_len\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合理性检查：为了确保训练集和验证集中数据量大于模型的上下文窗口，避免出现训练/验证错误\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"ctx_len\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['ctx_len']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"ctx_len\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['ctx_len']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- 我们使用相对较小的批次大小来减少计算资源的需求，并且因为数据集本身起初就非常小。\n",
    "- 例如，Llama 2 7B就是使用1024的批次大小进行训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- 一个可选的检查，以确认数据是否已正确加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- 另一个可选的检查，以确认标记大小是否在预期的范围内："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel() # 使用numel()函数统计一个batch中的token数量\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- 接下来，我们实现一个实用工具函数来计算给定批次的交叉熵损失。\n",
    "- 此外，我们实现了第二个实用工具函数，用于计算数据加载器中用户指定数量批次的总损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)\n",
    "    logits = logits.flatten(0, 1)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None): # num_batches为计算损失的批次范围\n",
    "    total_loss = 0.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 取num_batches和len(data_loader)两者较小值以匹配data_loader中的总批次数量\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- 如果你拥有一台装有支持CUDA的GPU的计算机，大型语言模型（LLM）将在GPU上进行训练，无需对代码做任何更改。\n",
    "- 通过`device`设置，我们确保数据被加载到与LLM模型相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # 对于nn.Module类的模型，不需要执行model = model.to(device)这样的赋值操作。\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # 出于代码结果的可复现性的考虑，显式地设定manual_seed\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "## 5.2 训练一个大型语言模型（LLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- 在本节中，我们最终实现了训练大型语言模型（LLM）的代码。\n",
    "- 我们专注于一个简单的训练函数（如果你对使用更先进的技术增强这个训练函数感兴趣，例如学习率预热、余弦退火和梯度裁剪，请参考[Appendix D](../../appendix-D/03_main-chapter-code))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context):\n",
    "    # 初始化列表以跟踪损失和已观察到的token\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 主要的训练步骤\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 将模型设置为训练模式\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # 每个epoch开始之前重新设置梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # 计算损失梯度\n",
    "            optimizer.step() # 利用损失梯度更新模型参数\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 可选的验证评估步骤\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # 在每个epoch完成后打印一个生成的文本示例\n",
    "        generate_and_print_sample(\n",
    "            model, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # 简洁的打印格式\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- 现在，让我们使用上面定义的训练函数来训练大型语言模型（LLM）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.742, Val loss 9.856\n",
      "Ep 1 (Step 000005): Train loss 7.773, Val loss 8.053\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010): Train loss 6.346, Val loss 6.804\n",
      "Ep 2 (Step 000015): Train loss 5.957, Val loss 6.579\n",
      "Every effort moves you, the,,,,,,,,,,.                                     \n",
      "Ep 3 (Step 000020): Train loss 5.733, Val loss 6.530\n",
      "Ep 3 (Step 000025): Train loss 5.484, Val loss 6.774\n",
      "Every effort moves you, and I had to the to the to the the to the to the of the to the to the to the                           \n",
      "Ep 4 (Step 000030): Train loss 4.715, Val loss 6.692\n",
      "Ep 4 (Step 000035): Train loss 4.241, Val loss 6.253\n",
      "Every effort moves you, as a, and the picture.                                          \n",
      "Ep 5 (Step 000040): Train loss 3.593, Val loss 6.186\n",
      "Every effort moves you know the \"Oh, and he had to me--as his last word.     \"Oh, and. Gisburn, and I had always at my elbow and he had been, and down the room, I had\n",
      "Ep 6 (Step 000045): Train loss 3.124, Val loss 6.196\n",
      "Ep 6 (Step 000050): Train loss 2.442, Val loss 6.234\n",
      "Every effort moves you know,\" was not that he was. \"I-chairs forward. I had been his eyes's an the fact, and he said. I was his pictures with a little him. \" his pictures, and down the room, in his\n",
      "Ep 7 (Step 000055): Train loss 2.204, Val loss 6.281\n",
      "Ep 7 (Step 000060): Train loss 1.542, Val loss 6.258\n",
      "Every effort moves you know,\" was not that my hostess was \"I told me.  \"I looked up, and to me to have to see a smile behind his pictures.  \"I looked up his pictures with a little under. \"I\n",
      "Ep 8 (Step 000065): Train loss 1.093, Val loss 6.362\n",
      "Ep 8 (Step 000070): Train loss 0.860, Val loss 6.426\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again. Gisburn's open countenance. \"It's his ridiculous modesty, and were amusing himself by holding\n",
      "Ep 9 (Step 000075): Train loss 0.604, Val loss 6.480\n",
      "Ep 9 (Step 000080): Train loss 0.429, Val loss 6.542\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.275, Val loss 6.636\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # 绘制随着训练进行（epoch值增大）训练集损失和验证集损失的变化情况\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # 创建第二个x轴用于显示可观察的tokens\n",
    "    ax2 = ax1.twiny()  # 创建一个共享相同y轴的第二个x轴\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # 用于对齐刻度的不可见图表\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # 调整布局以节省空间\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- 从上面的结果来看，我们可以看到模型开始生成无法理解的单词串，而到了后期，它能够产生语法上或多或少正确的句子。\n",
    "- 然而，根据训练集和验证集的损失情况，我们可以看到模型开始过拟合。\n",
    "- 如果我们检查它在训练结束时写的几段文本，我们会发现自己它们与训练集中的内容几乎一字不差——它只是简单地记住了训练数据。\n",
    "- 稍后，我们将讨论一些解码策略，可以在一定程度上减轻这种记忆现象。\n",
    "- 请注意，这里的过拟合是因为我们有一个非常非常小的训练集，而且我们多次迭代它。\n",
    "  - 这里的大型语言模型训练主要是出于教育目的；我们主要想看到模型能够学习产生连贯的文本。\n",
    "  - 我们不会花费数周或数月的时间在昂贵的硬件上训练这个模型，我们将在后续加载预训练的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**如果您对使用更先进的技术来增强这个训练函数感兴趣，例如学习率预热、余弦退火和梯度裁剪，请参考[附录D](../../appendix-D/03_main-chapter-code)。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**如果您对更大的训练数据集和更长时间的训练感兴趣，请查看 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 解码策略以控制随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- 使用相对较小的大型语言模型（如我们上面训练的GPT模型），推理过程相对廉价，因此如果您在上面使用GPU进行了训练，那么在推理时就不需要使用GPU。\n",
    "- 使用我们之前在简单训练函数中使用的`generate_text_simple`函数（来自上一章），我们可以一次生成一个单词（或标记）的新文本。\n",
    "- 如5.1.2节所解释的，下一个生成的标记是词汇表中所有标记中对应最大概率分数的标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- 即使我们多次执行上面的`generate_text_simple`函数，大型语言模型（LLM）始终会生成相同的输出。\n",
    "- 现在我们引入两个概念，所谓的解码策略，来修改`generate_text_simple`：*温度缩放*和*top-k*采样。\n",
    "- 这将允许模型控制生成文本的随机性和多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 温度缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- 之前，我们总是使用`torch.argmax`采样最大概率的标记作为下一个标记。\n",
    "- 为了增加多样性，我们可以使用`torch.multinomial(probs, num_samples=1)`从概率分布中采样下一个标记。\n",
    "- 在这里，每个索引被选中的机会与其在输入张量中的概率相对应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- 这里是一个关于生成下一个标记的小回顾，假设一个非常小的词汇表，仅用于说明目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# 假设input是 \"every effort moves you\", 模型返回的logits值为下面tensor中的数值:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# 下一个标记:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)] # 使用torch.multinomial函数从probas中进行了1000次采样\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample)) # 使用torch.bitcount函数统计每个token的采样数量\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- 我们不是通过`torch.argmax`来确定最可能的标记，而是使用`torch.multinomial(probas, num_samples=1)`从softmax分布中采样来确定最可能的标记。\n",
    "- 为了说明，让我们看看当我们使用原始的softmax概率采样1000次下一个标记时会发生什么："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- 我们可以通过一个称为温度缩放的概念来控制分布和选择过程。\n",
    "- “温度缩放”只是将logits除以一个大于0的数字的高级说法。\n",
    "- 大于1的温度值将在应用softmax后导致更均匀分布的标记概率。\n",
    "- 小于1的温度值将在应用softmax后导致更自信（更尖锐或更高峰）的分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5, 0.7]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, T in enumerate(temperatures):\n",
    "    # 条形图的绘制，ax.bar()函数里面的参数分别为条形的x轴位置、高度、宽度、图例标签\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- 我们可以看到，通过温度0.1进行重新缩放会得到一个更尖锐的分布，接近于`torch.argmax`，以至于最可能的单词几乎总是被选中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- 通过`temperature=5`重新缩放的概更加均匀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- 假设大型语言模型（LLM）的输入是“every effort moves you”，使用上述方法有时会产生无意义的文本，例如“every effort moves you pizza”，这种情况发生的频率是3.2%（在1000次中有32次）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- 为了能够使用更高的温度来增加输出的多样性，并降低无意义句子出现的概率，我们可以将采样的标记限制在最可能的前k个标记中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- 在代码中，我们可以如下实现这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 修改文本生成函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- 前两个小节介绍了温度采样和top-k采样。\n",
    "- 让我们使用这两个概念来修改我们之前用于通过大型语言模型（LLM）生成文本的`generate_simple`函数，创建一个新的`generate`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
    "\n",
    "    # 循环与之前相同：获取logits，并仅关注最后一步。\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 使用top_k采样对logits值进行过滤\n",
    "        if top_k is not None:\n",
    "            # 仅保留top_k的值\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # 使用温度缩放\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # 使用softmax函数得到概率\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # 从概率分布中采样\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # 否则和之前的generate_simple函数中的处理相同，使用argmax函数取得概率最大的token\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        # 和之前相同的序列拼接处理\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to go a little wild--I was no struck by holding enough--so it was no\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=20,\n",
    "    context_size=GPT_CONFIG_124M[\"ctx_len\"],\n",
    "    top_k=10,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 在PyTorch中加载和保存模型权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- 训练大型语言模型（LLM）需要昂贵的计算资源，因此能够保存和加载LLM权重至关重要。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- PyTorch推荐的方式是保存模型权重，即所谓的`state_dict`，通过应用`torch.save`函数到`.state_dict()`方法来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- 然后我们可以按照以下方式将模型权重加载到一个新的`GPTModel`模型实例中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(GPT_CONFIG_124M)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval();\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pth'"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- 通常的做法是使用自适应优化器（如Adam或AdamW）而不是常规的SGD来训练大型语言模型（LLM）\n",
    "- 这些自适应优化器会为每个模型权重存储额外的参数，因此如果我们计划稍后继续预训练，保存它们也是有意义的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 从Open AI加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- 之前，我们仅出于教育目的使用一本非常小的短篇小说书训练了一个小型的GPT-2模型。\n",
    "- 感兴趣的读者还可以在[../03_bonus_pretraining_on_gutenberg](03_bonus_pretraining_on_gutenberg)中找到在完整的古登堡计划书库上进行更长时间预训练的信息。\n",
    "- 幸运的是，我们不需要花费数万到数十万美元在大型预训练语料库上预训练模型，而是可以直接加载由OpenAI提供的预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "- 从Hugging Face Hub加载权重的方法，请参见[../02_alternative_weight_loading](../02_alternative_weight_loading)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- 首先，一些模板代码用于从OpenAI下载文件并将权重加载到Python中。\n",
    "- 由于OpenAI使用了[TensorFlow](https://www.tensorflow.org/)，我们将不得不安装并使用TensorFlow来加载权重；[tqdm](https://github.com/tqdm/tqdm) 是一个进度条库。\n",
    "- 取消注释并运行下一个单元格以安装所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "tqdm version: 4.64.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有关的函数导入\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "- 然后我们可以按照以下方式下载具有1.24亿参数的模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models\\124M\\checkpoint: 100%|██████████| 77.0/77.0 [00:00<?, ?iB/s]\n",
      "models\\124M\\encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 635kiB/s] \n",
      "models\\124M\\hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 89.9kiB/s]\n",
      "models\\124M\\model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [03:10<00:00, 2.62MiB/s]   \n",
      "models\\124M\\model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 5.21MiB/s]\n",
      "models\\124M\\model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 420kiB/s]  \n",
      "models\\124M\\vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 406kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "hparams, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b20f0",
   "metadata": {},
   "source": [
    "note: 如果出现报错：requests.exceptions.SSLError: Max retries exceeded weith url \n",
    "更改urllib3的版本\n",
    "`pip install urllib3==1.25.11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- 另外，\"355M\"、\"774M\" 和 \"1558M\" 也是支持的 `model_size` 参数。\n",
    "- 这些不同大小的模型之间的差异在下面的图表中进行了总结："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- 上面，我们将124M GPT-2模型权重加载到了Python中，但我们还需要将它们转移到我们的`GPTModel`实例中。\n",
    "- 首先，我们初始化一个新的GPTModel实例。\n",
    "- 请注意，原始的GPT模型在多头注意力模块的查询、键和值矩阵的线性层中使用了带偏置向量的初始化，这是不必要的，也不推荐；然而，为了能够正确加载权重，我们也必须在我们的实现中通过设置`qkv_bias`为`True`来启用这些。\n",
    "- 我们还使用了原始GPT-2模型使用的`1024`上下文窗口长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型配置参数定义在一个字典中\n",
    "model_configs = {\n",
    "    \"gpt2-small\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# 复制基础配置，并使用特定的模型设置进行更新\n",
    "model_name = \"gpt2-small\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"ctx_len\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- 接下来需要将OpenAI的权重分配给我们的`GPTModel`实例中相应的权重张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # Weight tying\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- 如果模型加载正确，我们可以使用它结合我们之前的`generate`函数来生成新文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEW_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mctx_len\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(model, idx, max_new_tokens, context_size, temperature, top_k)\u001b[0m\n\u001b[0;32m      5\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mcontext_size:]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 使用top_k采样对logits值进行过滤\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding\\AILearning\\LLM\\LLM_Learning\\llms-from-scratch-cn\\ch05\\01_main-chapter-code\\previous_chapters.py:202\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[1;34m(self, in_idx)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[0;32m    201\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m in_idx\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 202\u001b[0m     tok_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     pos_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39min_idx\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m    204\u001b[0m     x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"ctx_len\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
   "metadata": {},
   "source": [
    "- 我们知道模型权重加载正确，因为模型能够生成连贯的文本；如果我们犯了哪怕很小的错误，模型也无法做到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
   "metadata": {},
   "source": [
    "- 有关从Hugging Face Hub加载权重的替代方法，请参考[../02_alternative_weight_loading](../02_alternative_weight_loading)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
   "metadata": {},
   "source": [
    "## 总结和要点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
   "metadata": {},
   "source": [
    "- 查看包含独立训练脚本的[gpt_train.py](gpt_train.py)文件。\n",
    "- [gpt_generate.py](gpt_generate.py)文件从OpenAI加载预训练权重，并根据提示生成文本。\n",
    "- 你可以在[exercise-solutions.ipynb](exercise-solutions.ipynb)中找到练习题的解答。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
